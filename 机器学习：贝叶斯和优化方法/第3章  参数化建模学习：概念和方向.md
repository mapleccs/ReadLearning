## 第3章  参数化建模学习：概念和方向

### 3.1  引言

参数化建模是贯穿本书的一个主题。许多章节集中在这一重要问题的不同方面。本章提供了当参数模型被用于描述可用数据时与学习任务相关的一些基本定义和概念。

正如在第 1 章中已经指出的，机器学习的一大类问题最终等价于函数估计/逼近任务。在学习/训练阶段，通过挖掘可用训练数据集中的信息，从而＂习得＂某个函数。此函数将所谓的输入变量与输出变量相关联。一旦建立了该函数关系，基于从各个输入变量获得的测量值，就可以利用它来预测输出值，然后这些预测可以被用于决策阶段。

在参数化建模中，前述输入到输出的函数依赖是通过一组未知参数来定义的，这些参数的个数是固定且先验已知的。参数的值是未知的，必须根据现有的输入一输出观测值进行估计。与参数化方法相对，也存在所谓的非参数化方法。在这种方法中，仍然可能涉及参数以建立输入一输出关系，但它们的数量不是固定的，而是取决于数据集的大小，随着观测值的数目而增长。本书也将讨论非参数化方法（如第 11 章和第 13 章），但本章的重点是参数化方法。

存在两条技术路线来处理涉及参数的末知值所引发的不确定性。根据第一条路线，参数被视为确定性非随机变量。学习的任务是估计它们的未知值。对于每一个参数，都会得到单一估计值。另一种方法具有更强的统计味道。将未知参数视为随机变量，学习的任务是推断出相关的概率分布。一旦学习/推断了分布，就可以使用它们进行预测。这两种方法都将在本章中介绍，稍后将在本书的各个章节中进行更详细的讨论。

本章给出了两个主要的机器学习任务，即回归和分类，并揭示了处理这些问题的主要方向。本章还介绍和讨论了与参数估计任务相关的各种问题，如估计效率，偏差－方差困境，过拟合和维数灾难等。这一章也可以看作本书其余部分的路线图。然而，我们并不是仅仅以一种相当＂枯燥＂的方式呈现主要思想和方向，而是选择采用简单的模型和技术来处理复杂的任务，以便读者更好地理解本章主题。本章力图将注意力放在科学思想，而非代数运算和数学细节方面，这是因为在后面章节的描述中，我们将不可避免地在更大程度上使用这些思想。

本章介绍并讨论了最小二乘（LS），极大似然（ML），正则化以及贝叶斯推断等技术，并致力于帮助读者掌握本书想展示的大图景。因此，本章还可以作为机器学习领域参数化建模任务的一个概观介绍。

### 3.2  参数估计：确定性观点

估算未知参数向量 $\boldsymbol{\theta}$ 的值已经成为许多应用领域关注的焦点。例如，在大学的最初几年，任何学生必须学习的内容之一就是所谓的曲线拟合问题。即给定一组数据点，找到能＂拟合＂这些数据的曲线或曲面。通常的方法是采用函数形式，如线性函数或二次函数，并尝试估计相关的未知参数，以使相应函数的图形＂穿过＂给定数据，并尽可能接近它们在空间中的位置。图 3.1a 和图 3.1b 是两个这样的例子。数据位于 $\mathbb{R}^2$ 空间，即给定一组点 $\left(y_n, x_n\right), n=1,2, \cdots, N$ 。图3.1a 所对应的曲线采用的函数形式为

$$
\begin{equation}
\label{qua1}
	y=f_\theta(x)=\theta_0+\theta_1 x
\end{equation}
$$

相应地

，图 3.1b 采用的函数形式为
$$
\begin{equation}
\label{qua2}
y=f_\theta(x)=\theta_0+\theta_1 x+\theta_2 x^2
\end{equation}
$$

未知参数向量分别为 $\boldsymbol{\theta}=\left[\theta_0, \theta_1\right]^{\mathrm{T}}$ 和 $\boldsymbol{\theta}=\left[\theta_0, \theta_1, \theta_2\right]^{\mathrm{T}}$ 。在这两种情况下，对应灰色曲线的参数值都比对应黑色曲线的参数值更为适合。这两种情况下的曲线拟合任务包括两个步骤：1）采用一种我们认为更适合手头数据的特定参数函数形式；2）估计未知参数的值，以获得＂好＂的拟合。

![图3.1 拟合一个线性函数（a）和拟合一个二次函数（b）。灰线表示更优的拟合](https://zhihuiss2024.oss-cn-nanjing.aliyuncs.com/img/image-20250509094657582.png)更一般，更形式化地，参数估计任务可以定义如下。给定一组数据点 $\left(y_n, \boldsymbol{x}_n\right), y_n \in$ $\mathbb{R}, \boldsymbol{x}_n \in \mathbb{R}^l, n=1,2, \cdots, N$ 和一个参数化 ${ }^{\ominus}$ 的函数集
$$
\begin{equation}
\label{qua3}
\mathcal{F}:=\left\{f_\theta(\cdot): \theta \in \mathcal{A} \subseteq \mathbb{R}^K\right\}
\end{equation}
$$

在 $\mathcal{F}$ 中找到这样一个函数，不妨记为 $f(\cdot):=f_{\boldsymbol{\theta}}{ }_{\boldsymbol{*}}(\cdot)$ ，对给定的 $\boldsymbol{x} \in \mathbb{R}^l, f(\boldsymbol{x})$ 是对应 $y \in \mathbb{R}$ 值的最佳逼近。集合 $\mathcal{A}$ 是一个约束集，如果我们希望约束 $K$ 个未知参数位于 $\mathbb{R}^K$ 中一个特定区域内。在机器学习中，将参数限制在参数空间 $\mathbb{R}^K$ 的一个子集内几乎是无处不在的。我们将在本章稍后（ 3.8 节）介绍约束优化。首先把 $y$ 当作一个实变量来讨论，即 $y \in \mathbb{R}$ ，当我们继续前进并更好地理解了各种＂秘密＂时，我们将允许它位于更高维度的欧几里得空间。 $\boldsymbol{\theta}_*$ 的值是由估计过程产生的。图 3.1a 和图 3.1b 中定义灰色曲线的 $\boldsymbol{\theta}_*$ 的取值分别为
$$
\begin{equation}
\label{qua4}
\boldsymbol{\theta}_*=[-0.5,1]^{\mathrm{T}}, \quad \boldsymbol{\theta}_*=[-3,-2,1]^{\mathrm{T}}
\end{equation}
$$

选择 $\mathcal{F}$ 并不是一件容易的事。对于图3.1中的数据，我们是比较＂幸运＂的。首先，数据位于二维空间，我们可以进行可视化。其次，数据沿着某个曲线散布，且曲线的形状我们很熟悉。因此。简单的观察就可以为这两种情形中的每一种提出合适的函数族。很明显，现实生活远没有这么简单。在大多数实际应用中，数据位于高维空间中，并且曲面 （对于维数大于 3 的空间是超曲面）的形状可能非常复杂。因此，决定函数形式（如线性，二次等）的 $\mathcal{F}$ 的选择并不容易。在实践中，必须使用尽可能多的先验信息，这些信息与生成数据的物理机制有关，而且通常使用不同的函数族，最后根据某个选择标准保留性能最好的函数族。

在采纳了某个参数化的函数族 $\mathcal{F}$ 后，还必须对未知参数进行估计。为此，必须有一个拟合好坏的度量标准。比较经典的方法是采用**损失函数**，它量化了 $y$ 的实测值与由相应的测量值 $\boldsymbol{x}$ 得到的预测值 $f_{\boldsymbol{\theta}}(\boldsymbol{x})$ 之间的偏差/误差。更形式化地，我们采用一个非负的（损失）函数

$$
\mathcal{L}(\cdot, \cdot): \mathbb{R} \times \mathbb{R} \longmapsto[0, \infty)
$$


进而计算出 $\boldsymbol{\theta}_*$ ，使得在所有数据点上的总损失，或者说代价最小，即

$$
\begin{equation}
\label{qua5}
f(\cdot):=f_{\boldsymbol{\theta}_*}(\cdot): \boldsymbol{\theta}_*=\arg \min _{\theta \in \mathcal{A}} J(\boldsymbol{\theta})
\end{equation}
$$


其中

$$
\begin{equation}
\label{qua6}
J(\boldsymbol{\theta}):=\sum_{n=1}^N \mathcal{L}\left(y_n, f_{\boldsymbol{\theta}}\left(\boldsymbol{x}_n\right)\right)
\end{equation}
$$


这里假设其存在最小值。注意，在一般情况下，可能有多个最优的 $\boldsymbol{\theta}_*$ ，这取决于 $J(\boldsymbol{\theta})$ 的形状。

随着本书内容的深入，我们将会遇到不同的损失函数和不同的参数化函数族。为了简单起见，在本章的其余部分，我们将固定使用平方误差损失函数

$$
\mathcal{L}\left(y, f_{\boldsymbol{\theta}}(\boldsymbol{x})\right)=\left(y-f_{\boldsymbol{\theta}}(\boldsymbol{x})\right)^2
$$

及线性函数族。

平方误差损失函数被归功于著名的数学家卡尔•弗雷德里希•高斯，他在1795年18岁时提出了 LS 方法的基本原理。在 1805 年，阿德里安－玛丽•勒让德独立工作并首次发表了这种方法。高斯在1809年发表了该工作。该方法在预测小行星谷神星位置时得到了验证。从那时起，平方误差损失函数反复出现在所有的科学领域，即使没有被直接使用，它也是最常见的性能对比标准，常被拿来和更现代的替代方案进行比较。这种成功得益于该损失函数所具有的一些优良特性，我们将在本书中继续探讨。

组合选择线性与平方损失函数简化了代数推导，因此非常适合教学，可方便地向新手介绍参数估计领域的一些关键点。此外，理解线性是非常重要的。非线性的任务在大多数情况下最终会转化为线性问题。以式$(2)$中的非线性模型为例，考虑变换

$$
\begin{equation}
\label{qua7}
\mathbb{R} \ni x \longmapsto \phi(x):=\left[\begin{array}{c}
x \\
x^2
\end{array}\right] \in \mathbb{R}^2
\end{equation}
$$

这时，式$(2)$变为
$$
\begin{equation}
\label{qua8}
y=\theta_0+\theta_1 \phi_1(x)+\theta_2 \phi_2(x)
\end{equation}
$$


也就是说，对 $\boldsymbol{x}$ 的二维图像 $\boldsymbol{\phi}(x)$ 的分量 $\phi_k(x)$，$k=1,2$ 来说，现在的模型就是线性的。事实上，这个简单的技巧是许多非线性方法的核心，这些方法将在本书后面讨论。毫无疑问，这个过程可以推广到任何数目（比如 $K$ ）的函数 $\phi_k(\boldsymbol{x}), k=1,2, \cdots, K$ 。而且不止单项式，其他类型的非线性函数也可以使用，如指数函数，样条函数，小波函数等。尽管输入－输出所依赖的模型本质上是非线性的，我们仍然认为这类模型是线性的，因为它保持了对所涉及的未知参数 $\theta_k, k=1,2, \cdots, K$ 的线性。为了使我们的讨论更简单，在这一章的其余部分我们将固定使用线性函数，但这里所说的一切也都适用于非线性问题。所需要的只是将 $\boldsymbol{x}$ 替换为 $\boldsymbol{\phi}(\boldsymbol{x}):=\left[\phi_1(\boldsymbol{x}), \cdots, \phi_K(\boldsymbol{x})\right]^{\mathrm{T}} \in \mathbb{R}^K$ 。

后续我们将提供两个例子来展示参数化建模的使用。这些例子是通用的，可以代表很广泛的一类问题。

### 3.3  线性回归

在统计学中，人们创造了＂回归＂一词来定义对随机变量之间的关系进行建模的任务。随机因变量 y 可看作被一组随机变量 $\mathrm{x}_1, \mathrm{x}_2, \cdots, \mathrm{x}_l$＂激活＂时一个系统的响应，这组随机变量是某个随机向量 $\mathbf{x}$ 的各个分量。该关系是通过一个附加扰动或噪声项 $\eta$ 来建模的。图3.2给出了关联相应随机变量的框图。噪声变量 $\eta$ 是一个未观测的随机变量。给定一组测量值/观测值 $\left(y_n, x_n\right) n=1,2, \cdots, N$ ，回归任务的目标是估计参数向量 $\boldsymbol{\theta}$ 。这组给定测量值/观测值也被称为训练数据集。因变量通常称为输出变量，向量 $\mathbf{x}$ 称为输入向量或回归量。如果我们将系统建模为线性组合，即依赖关系为

$$
\begin{equation}
\label{qua9}
\mathbf{y}=\theta_0+\theta_1 \mathbf{x}_1+\cdots+\theta_l \mathbf{x}_l+\eta=\theta_0+\boldsymbol{\theta}^{\mathrm{T}} \mathbf{x}+\eta
\end{equation}
$$

![图3.2 回归模型中输入－输出之间关系的框图](https://zhihuiss2024.oss-cn-nanjing.aliyuncs.com/img/image-20250509112651369.png)

参数 $\theta_0$ 称为偏置或截距。通常，这一项被参数向量 $\boldsymbol{\theta}$ 吸收，即通过给向量 $\mathbf{x}$ 的维数加 1并令其最后一个元素为常数 1 实现。事实上，我们有
$$
\theta_0+\boldsymbol{\theta}^{\mathrm{T}} \mathbf{x}+\eta=\left[\boldsymbol{\theta}^{\mathrm{T}}, \theta_0\right]\left[\begin{array}{c}
\mathbf{x} \\
1
\end{array}\right]+\eta
$$


从现在开始，回归模型可写作

$$
\begin{equation}
\label{qua10}
\mathrm{y}=\boldsymbol{\theta}^{\mathrm{T}} \mathbf{x}+\eta
\end{equation}
$$


除非另有说明，这种写法表示偏置项已被 $\boldsymbol{\theta}$ 和 $\mathbf{x}$ 吸收，且向量 $\mathbf{x}$ 已经增加了额外分量 1 。因为噪声变量是未观测的，我们需要一个模型，对给定的随机变量 $\mathbf{x}$ 的一个观测值它能够预测 y 的输出值。

在线性回归中，给定 $\boldsymbol{\theta}$ 的一个估计 $\hat{\boldsymbol{\theta}}$ ，我们采用以下预测模型

$$
\begin{equation}
\label{qua11}
\hat{y}=\hat{\theta}_0+\hat{\theta}_1 x_1+\cdots+\hat{\theta}_l x_l:=\hat{\boldsymbol{\theta}}^{\mathrm{T}} \boldsymbol{x}
\end{equation}
$$


利用平方误差损失函数，将估计值 $\hat{\boldsymbol{\theta}}$ 取为 $\boldsymbol{\theta}$ ，它使得在所有观测量上 $\hat{y}_n$ 和 $y_n$ 之间的差的平方和最小。即关于 $\boldsymbol{\theta}$ ，极小化代价函数

$$
\begin{equation}
\label{qua12}
J(\boldsymbol{\theta})=\sum_{n=1}^N\left(y_n-\boldsymbol{\theta}^{\mathrm{T}} \boldsymbol{x}_n\right)^2
\end{equation}
$$


我们从不考虑约束开始讨论。因此，我们令 $\mathcal{A}=\mathbb{R}^K$ ，然后要寻找位于 $\mathbb{R}^K$ 中任意位置的解。关于 $\boldsymbol{\theta}$ 求导数（梯度）（参见附录 A ）并令其等于零向量 $\mathbf{0}$ ，可得（习题 3.1）

$$
\begin{equation}
\label{qua13}
\left(\sum_{n=1}^N \boldsymbol{x}_n \boldsymbol{x}_n^{\mathrm{T}}\right) \hat{\boldsymbol{\theta}}=\sum_{n=1}^N y_n \boldsymbol{x}_n
\end{equation}
$$


注意，左侧求和是 $N$ 个向量外积即 $\boldsymbol{x}_n \boldsymbol{x}_n^{\mathrm{T}}$ 之和，结果是一个 $(l+1) \times(l+1)$ 的矩阵。由线性代数知识可知，我们至少需要 $N=l+1$ 才能保证矩阵可逆，当然前提是向量线性无关 （参见［35］）。

对于那些仍不很熟悉使用向量的人，请注意式$(13)$只是标量情况的推广。例如，在 ＂标量＂世界中，输人一输出对将由标量 $\left(y_n, x_n\right)$ 组成，而未知参数也会是一个标量 $\theta$ 。代价函数为 $\sum_{n=1}^N\left(y_n-\theta x_n\right)^2$ 。对其求导，令导数等于 0 ，得到
$$
\left(\sum_{n=1}^N x_n^2\right) \hat{\theta}=\sum_{n=1}^N y_n x_n, \text { 或 } \hat{\theta}=\frac{\sum_{n=1}^N y_n x_n}{\sum_n^N x_n^2}
$$


一种更常用的得到式$(13)$的方法是通过所谓的输人矩阵 $X$ 来表示先前得到的关系， $X$ 为 $N \times(l+1)$ 的矩阵，它的各行是（扩展的）回归向量 $\boldsymbol{x}_n^{\mathrm{T}}, n=1,2, \cdots, N, X$ 表示为

$$
\begin{equation}
\label{qua14}
X:=\left[\begin{array}{c}
\boldsymbol{x}_1^{\mathrm{T}} \\
\boldsymbol{x}_2^{\mathrm{T}} \\
\vdots \\
\boldsymbol{x}_N^{\mathrm{T}}
\end{array}\right]=\left[\begin{array}{cccc}
x_{11} & \cdots & x_{1 l} & 1 \\
x_{21} & \cdots & x_{2 l} & 1 \\
\vdots & & \vdots & \\
x_{N 1} & \cdots & x_{N l} & 1
\end{array}\right]
\end{equation}
$$


这样，很容易看出式$(13)$可以写成

$$
\begin{equation}
\label{qua15}
\left(X^{\mathrm{T}} X\right) \hat{\boldsymbol{\theta}}=X^{\mathrm{T}} \boldsymbol{y}
\end{equation}
$$


其中

$$
\begin{equation}
\label{qua16}
\boldsymbol{y}:=\left[y_1, y_2, \cdots, y_N\right]^{\mathrm{T}}
\end{equation}
$$


实际上

$$
X^{\mathrm{T}} X=\left[x_1, \cdots, x_N\right]\left[\begin{array}{c}
\boldsymbol{x}_1^{\mathrm{T}} \\
\vdots \\
\boldsymbol{x}_N^{\mathrm{T}}
\end{array}\right]=\sum_{n=1}^N \boldsymbol{x}_n \boldsymbol{x}_n^{\mathrm{T}}
$$


且类似有

$$
X^{\mathrm{T}} \boldsymbol{y}=\left[\boldsymbol{x}_1, \cdots, \boldsymbol{x}_N\right]\left[\begin{array}{c}
y_1 \\
\vdots \\
y_N
\end{array}\right]=\sum_{n=1}^N y_n x_n
$$


因此，最终 LS 估计为

$$
\begin{equation}
\label{qua17}
\hat{\boldsymbol{\theta}}=\left(X^{\mathrm{T}} X\right)^{-1} X^{\mathrm{T}} \boldsymbol{y}: \quad \text { LS估计 }
\end{equation}
$$

当然，此时假设 $\left(X^{\mathrm{T}} X\right)^{-1}$ 存在。

换句话说，得到的参数向量的估计是由一个线性方程组给出的。当应用于线性模型时，这是平方误差损失函数的一个主要优点。此外，只要 $(l+1) \times(l+1)$ 矩阵 $X^{\mathrm{T}} X$ 是可逆的，那么解就是唯一的。唯一性是因为平方误差代价函数的和的图形是抛物形。图3.3展示了二维时的情形。很容易看出该图形有唯一的最小值，这是因为平方误差代价函数的和是严格凸函数。与损失函数的凸性有关的问题将在第 8 章中详细讨论。

![图3.3 平方误差损失在点 $\boldsymbol{\theta}_*$ 处有唯一最小值](https://zhihuiss2024.oss-cn-nanjing.aliyuncs.com/img/image-20250509133628992.png)

例3.1 考虑以下模型描述的系统：

$$
\begin{equation}
\label{qua18}
y=\theta_0+\theta_1 x_1+\theta_2 x_2+\eta:=[0.25,-0.25,0.25]\left[\begin{array}{c}
x_1 \\
x_2 \\
1
\end{array}\right]+\eta
\end{equation}
$$
其中 $\eta$ 是零均值，方差 $\sigma^2=1$ 的高斯随机变量。可以观察到，由于噪声，生成的数据围绕如下二维空间中的平面（参见图3.4c）散布

$$
\begin{equation}
\label{qua19}
f(\boldsymbol{x})=\theta_0+\theta_1 x_1+\theta_2 x_2
\end{equation}
$$


假设随机变量 $x_1$ 和 $x_2$ 相互独立，且在区间 $[0,10]$ 上均匀分布。而且，两个变量都与噪声变量 $\eta$ 无关。我们为这三个随机变量（即 $x_1, ~ x_2, ~ \eta$ ）的每一个都生成 $N=50$ 个独立同分布的点 ${ }^{\ominus}$ 。对于每一个三元组，我们利用式$(18)$生成对应的 $y$ 值。这样，就生成了点 $\left(y_n, \boldsymbol{x}_n\right), n=1,2, \cdots, 50$ ，其中每个观测值 $\boldsymbol{x}_n$ 都位于 $\mathbb{R}^2$ 中。将它们作为训练点，求出如下线性模型的参数的 LS 估计

$$
\begin{equation}
\label{qua20}
\hat{y}=\hat{\theta}_0+\hat{\theta}_1 x_1+\hat{\theta}_2 x_2
\end{equation}
$$

然后用 $\sigma^2=10$ 重复该实验。注意，式$(20)$定义的平面通常与原来的式$(19)$不同。

![image-20250509134012209](https://zhihuiss2024.oss-cn-nanjing.aliyuncs.com/img/image-20250509134012209.png)

通过求解 $3 \times 3$ 的线性方程组，对这两种情况分别得到了 LS 最优估计的值
$$
(a)\hat{\theta}_0=0.028, \hat{\theta}_1=0.226, \hat{\theta}_2=-0.224
\\
(b)\hat{\theta}_0=0.914, \hat{\theta}_1=0.325, \hat{\theta}_2=-0.477
$$
图 3.4 a 和图 3.4 b 显示了恢复的平面。可以观察到，图 3.4 a 的情况对应于一个方差较小的噪声变量，所得到的平面与图 3.4 b 相比，更接近于数据点。

**附注3.1**

- 点集 $\left(\hat{y}_n, x_{n 1}, \cdots, x_{n l}\right), n=1,2, \cdots, N$ ，位于 $\mathbb{R}^{l+1}$ 空间的一个超平面上。等同地，正如前面解释的那样，如果将 $\theta_0$ 吸收进 $\boldsymbol{\theta}$ 中，则此时它们位于过原点的一个超平面上，即扩展空间 $\mathbb{R}^{l+2}$ 的一个线性子空间。
- 请注意，即使真实系统的结构不服从式$(9)$中的线性模型，仍然可以使用式$(11)$中的预测模型。例如， $y$ 和 $\mathbf{x}$ 之间的真实依赖关系可能是非线性的。但是，在这种情况下，根据式$(11)$中的模型，对 $y$ 的预测可能不会太好。这完全取决于我们所采用的模型与生成数据的系统的真实结构之间的偏差。
- 模型的预测性能也取决于噪声变量的统计性质。这是一个重要的问题。我们将在后面看到，根据噪声变量的统计性质，一些损失函数和方法可能比别的更合适。
- 前面的两个附注表明，为了量化估计量的性能，必须采用一些相关的判别标准。在 3.9 节中，我们将介绍一些理论方面的内容，涉及与估计量性能相关的某些方面。

### 3.4  分类